{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/kashishparmar02/social-media-sentiments-analysis-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentimentAnalysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Timestamp', 'User', 'Platform', 'Hashtags', 'Retweets', 'Likes', 'Country', 'Year', 'Month', 'Day', 'Hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "analyzer = SentimentIntensityAnalyzer() \n",
    "def classify_sentiment(text, pos_threshold, neg_threshold):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if score['compound'] > pos_threshold:\n",
    "        return \"Positive\"\n",
    "    elif score['compound'] < neg_threshold:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for sentiment in df['Sentiment']:\n",
    "    labels.append(classify_sentiment(sentiment, 0.05, -0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = 'Sentiment', inplace = True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['label'] = encoder.fit_transform(df['label'])\n",
    "\n",
    "#displaying the edited dataframe\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = negative;\n",
    "1 = neutral;\n",
    "2 = positive;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['label'] == 0]['Text'].count())\n",
    "print(df[df['label'] == 1]['Text'].count())\n",
    "print(df[df['label'] == 2]['Text'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(df['label'].value_counts(), labels=['Positive', 'Negative', 'Neutral'], autopct = '%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column with count of characters\n",
    "df['countCharacters'] = df['Text'].apply(len)\n",
    "\n",
    "#creating a new column with count of words\n",
    "df['countWords'] = df['Text'].apply(lambda i:len(nltk.word_tokenize(i)))\n",
    "#'word_tokenize' function takes a string of text as input and returns a list of words\n",
    "\n",
    "#creating a new column with count of sentences\n",
    "df['countSentences'] = df['Text'].apply(lambda i:len(nltk.sent_tokenize(i)))\n",
    "#'sent_tokenize' function takes a string of text as input and returns a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the summary of the 3 new column values\n",
    "df[['countCharacters', 'countWords', 'countSentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for negative\n",
    "df[df['label'] == 0][['countCharacters', 'countWords', 'countSentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for neutral\n",
    "df[df['label'] == 1][['countCharacters', 'countWords', 'countSentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for positive\n",
    "df[df['label'] == 2][['countCharacters', 'countWords', 'countSentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.histplot(df[df['label'] == 0]['countCharacters'], color = \"red\")\n",
    "sns.histplot(df[df['label'] == 1]['countCharacters'], color = \"yellow\")\n",
    "sns.histplot(df[df['label'] == 2]['countCharacters'], color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.histplot(df[df['label'] == 0]['countWords'], color = \"red\")\n",
    "sns.histplot(df[df['label'] == 1]['countWords'], color = \"yellow\")\n",
    "sns.histplot(df[df['label'] == 2]['countWords'], color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#stopwords = stopwords.words(\"English\")\n",
    "def transform_text (text):\n",
    "    \n",
    "    #converting to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    #tokenization\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    #removing special characters\n",
    "    removedSC = list()\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            removedSC.append(i)\n",
    "            \n",
    "    #updating the text after removed special characters\n",
    "    text = removedSC[:]\n",
    "    \n",
    "    #removing stop words and punctuation characters\n",
    "    removedSWPC = list()\n",
    "    for i in text:\n",
    "        #stopwords.words('english') is a function of 'nltk', returns list of english stop words\n",
    "        #string.punctuation is a part of 'string' module, containing the ASCII punctuation characters\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            removedSWPC.append(i)\n",
    "            \n",
    "    #updating the text after removed stop words and punctuation characters\n",
    "    text = removedSWPC[:]\n",
    "    \n",
    "    #stemming the data using 'PorterStemmer' algorithm.\n",
    "    #nltk module provides this class to use.\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = list()\n",
    "    for i in text:\n",
    "        stemmed.append(ps.stem(i))\n",
    "    text = stemmed[:]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transform_text(\"Hello world this is me typing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will create a new column to store the transformed text -> 'processed'\n",
    "df['processed'] = df['Text'].apply(transform_text)\n",
    "\n",
    "#displaying the edited dataframe with a new column 'processed'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a wordcloud for the positive messages\n",
    "pos = wc.generate(df[df['label'] == 2]['processed'].str.cat(sep=\" \"))\n",
    "\n",
    "#creating figure and displaying\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a wordcloud for the neutral messages\n",
    "neutral = wc.generate(df[df['label'] == 1]['processed'].str.cat(sep=\" \"))\n",
    "\n",
    "#creating figure and displaying\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a wordcloud for the negitive messages\n",
    "neg = wc.generate(df[df['label'] == 0]['processed'].str.cat(sep=\" \"))\n",
    "\n",
    "#creating figure and displaying\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the collection of text into a matrix of token counts\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the data of processed column\n",
    "X = cv.fit_transform(df['processed']).toarray()\n",
    "\n",
    "#printing size of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the values of the 'result' column\n",
    "y = df['label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the objects for the models\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for GaussianNB\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print(precision_score(y_test, y_pred1, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for MultinomialnNB\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(precision_score(y_test, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for BernoulliNB\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred3, ))\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(precision_score(y_test, y_pred3, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using 'TfidfVectorizer' for vectorization \n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "#transforming the data of processed column\n",
    "X = tf.fit_transform(df['processed']).toarray()\n",
    "\n",
    "#storing the values of the 'result' column\n",
    "y = df['label'].values\n",
    "\n",
    "#splitting the training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for GaussianNB\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(confusion_matrix(y_test, y_pred1))\n",
    "print(precision_score(y_test, y_pred1, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for MultinomialnNB\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(precision_score(y_test, y_pred2, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the dataset for BernoulliNB\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred3))\n",
    "print(confusion_matrix(y_test, y_pred3))\n",
    "print(precision_score(y_test, y_pred3, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm1 = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1.fit(X_train, y_train)\n",
    "y_pred4 = svm1.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred4))\n",
    "print(confusion_matrix(y_test, y_pred4))\n",
    "print(precision_score(y_test, y_pred4, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will pickle 2 files\n",
    "import pickle\n",
    "pickle.dump(tf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(gnb,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
